{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"toy_data.txt\",'r',encoding='utf-8')as fn:\n",
    "    lines = fn.readlines()\n",
    "#     train = lines[:200000]\n",
    "#     dev = lines[200000:400000]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = lines[1::2]\n",
    "train_y = lines[0::2]\n",
    "train_x = list(map(lambda x:x.replace(\"\\n\",\"\").strip(),train_x))\n",
    "train_y = list(map(lambda x:x.replace(\"\\n\",\"\").strip(),train_y))\n",
    "\n",
    "# dev_x = dev[1::2]\n",
    "# dev_y = dev[0::2]\n",
    "# dev_x = list(map(lambda x:x.replace(\"\\n\",\"\").strip(),dev_x))\n",
    "# dev_y = list(map(lambda x:x.replace(\"\\n\",\"\").strip(),dev_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "創建x和y的vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_x = set()\n",
    "vocab_y = set()\n",
    "train_x = list(map(lambda x:list(x),train_x))\n",
    "train_y = list(map(lambda x:list(x),train_y))\n",
    "# dev_x = list(map(lambda x:list(x),dev_x))\n",
    "# dev_y = list(map(lambda x:list(x),dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for s in train_x:\n",
    "    vocab_x.update(s)\n",
    "for s in train_y:\n",
    "    vocab_y.update(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('vocab_x','w',encoding='utf-8')as fno:\n",
    "    for w in vocab_x:\n",
    "        fno.write(w+\"\\n\")\n",
    "with open('vocab_y','w',encoding='utf-8')as fno:\n",
    "    for w in vocab_y:\n",
    "        fno.write(w+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2idx_x = dict()\n",
    "w2idx_y = dict()\n",
    "with open('vocab_x','r',encoding='utf-8')as fn:\n",
    "    lines = fn.readlines()\n",
    "    for idx,w in enumerate(lines):\n",
    "        w2idx_x[w[:-1]] = idx+2\n",
    "with open('vocab_y','r',encoding='utf-8')as fn:\n",
    "    lines = fn.readlines()\n",
    "    for idx,w in enumerate(lines):\n",
    "        w2idx_y[w[:-1]] = idx+2\n",
    "idx2w_x = dict()\n",
    "idx2w_y = dict()\n",
    "for w, idx in w2idx_x.items():\n",
    "    idx2w_x[idx] = w\n",
    "for w, idx in w2idx_y.items():\n",
    "    idx2w_y[idx] = w\n",
    "idx2w_x[0] = \"PAD\"\n",
    "idx2w_x[1] = \"EOS\"\n",
    "idx2w_y[0] = \"PAD\"\n",
    "idx2w_y[1] = \"EOS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x_temp = list(map(lambda x:list(map(lambda y:w2idx_x[y],x)),train_x))\n",
    "train_y_temp = list(map(lambda x:list(map(lambda y:w2idx_y[y],x)),train_y))\n",
    "with open('train_x_vec','w',encoding='utf-8')as fno:\n",
    "    for s in train_x_temp:\n",
    "        fno.write(str(s)+'\\n')\n",
    "with open('train_y_vec','w',encoding='utf-8')as fno:\n",
    "    for s in train_y_temp:\n",
    "        fno.write(str(s)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import shift\n",
    "from keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(data_file):\n",
    "    stories = []\n",
    "    with open(data_file,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = ast.literal_eval(line.strip())\n",
    "            stories.append(line)\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = read_data('train_x_vec')\n",
    "train_y = read_data('train_y_vec')\n",
    "target_y = list(map(lambda x: x+[EOS],train_y))\n",
    "train_y = list(map(lambda x: [EOS]+x,train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = sequence.pad_sequences(train_x, maxlen=100, dtype='int32',padding='post', truncating='post', value=0.)\n",
    "train_y = sequence.pad_sequences(train_y, maxlen=100, dtype='int32',padding='post', truncating='post', value=0.)\n",
    "target_y = sequence.pad_sequences(target_y, maxlen=100, dtype='int32',padding='post', truncating='post', value=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開始建構graph和training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "content_length=100\n",
    "encoding_dim = 512\n",
    "vocab_x_size = 2882+2\n",
    "vocab_y_size = 1416+2\n",
    "embedding_size = 100\n",
    "readout_size = embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_next_batch():\n",
    "    cur = 0\n",
    "    pre = 0\n",
    "    while(True):\n",
    "        cur = cur+batch_size\n",
    "        yield train_x[pre:cur],train_y[pre:cur],target_y[pre:cur]\n",
    "        pre = cur\n",
    "        cur = cur%448\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_gen = get_next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X = tf.placeholder(tf.int32, [batch_size, content_length])\n",
    "\n",
    "X_embeddings = tf.Variable(tf.random_uniform([vocab_x_size, embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "Y_embeddings = tf.Variable(tf.random_uniform([vocab_y_size, embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "X_inputs_embedded = tf.nn.embedding_lookup(X_embeddings, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_s = tf.Variable(tf.random_uniform([2*encoding_dim, 2*encoding_dim], -1.0, 1.0), dtype=tf.float32)\n",
    "W_b = tf.Variable(tf.random_uniform([2*encoding_dim,1,1], -1.0, 1.0), dtype=tf.float32)\n",
    "U_s = tf.Variable(tf.random_uniform([2*encoding_dim, 2*encoding_dim], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "def encoder():\n",
    "    with tf.variable_scope('encode') as scope:\n",
    "        X_lens = tf.reduce_sum(tf.sign(tf.abs(X)), 1)\n",
    "        gru_cell_fw = tf.contrib.rnn.GRUCell(encoding_dim)\n",
    "        gru_cell_bw = tf.contrib.rnn.GRUCell(encoding_dim)\n",
    "        outputs, output_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                        gru_cell_fw,\n",
    "                                        gru_cell_bw,\n",
    "                                        X_inputs_embedded,\n",
    "                                        sequence_length=X_lens,\n",
    "                                        dtype=tf.float32)\n",
    "        h = tf.concat(outputs,2)#[batch,time,2*encoding_dim]\n",
    "        h = tf.reshape(h,[batch_size*content_length,2*encoding_dim])#[batch*time,2*encoding_dim]\n",
    "        s = tf.concat(output_states,1)#[?,2*encoding_dim]\n",
    "        WH = tf.matmul(W_s,tf.transpose(h))#[2*encoding_dim,batch*time]\n",
    "        \n",
    "        WH = tf.reshape(WH,[2*encoding_dim,batch_size,content_length])#[2*encoding_dim,batch,time]\n",
    "        US = tf.matmul(U_s,tf.transpose(s))#[2*encoding_dim,?]\n",
    "        US = tf.expand_dims(US,2)#[2*encoding_dim,?,1]\n",
    "        \n",
    "        sGate = tf.sigmoid(WH+US+W_b)#[2*encoding_dim,batch,time]\n",
    "        \n",
    "        h = tf.reshape(h,[batch_size,content_length,2*encoding_dim])#[batch,time,2*encoding_dim]\n",
    "\n",
    "        sGate = tf.transpose(sGate,[1,2,0])#[batch,time,2*encoding_dim]\n",
    "        h_hat = h*sGate#[batch,time,2*encoding_dim]\n",
    "        \n",
    "        return h_hat,output_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_d = tf.Variable(tf.random_uniform([encoding_dim, encoding_dim], -1.0, 1.0), dtype=tf.float32)\n",
    "S_b = tf.Variable(tf.random_uniform([encoding_dim, 1], -1.0, 1.0), dtype=tf.float32)\n",
    "W_a = tf.Variable(tf.random_uniform([encoding_dim, encoding_dim], -1.0, 1.0), dtype=tf.float32)\n",
    "U_a = tf.Variable(tf.random_uniform([encoding_dim, 2*encoding_dim], -1.0, 1.0), dtype=tf.float32)\n",
    "V_a = tf.Variable(tf.random_uniform([encoding_dim, 1], -1.0, 1.0), dtype=tf.float32)\n",
    "W_r = tf.Variable(tf.random_uniform([2*readout_size, embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "U_r = tf.Variable(tf.random_uniform([2*readout_size, 2*encoding_dim], -1.0, 1.0), dtype=tf.float32)\n",
    "V_r = tf.Variable(tf.random_uniform([2*readout_size, encoding_dim], -1.0, 1.0), dtype=tf.float32)\n",
    "W_o = tf.Variable(tf.random_uniform([readout_size, vocab_y_size], -1.0, 1.0), dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def step(pre_w,pre_c,pre_s,h_hat):\n",
    "    with tf.variable_scope('step') as scope:\n",
    "        #pre_w = [batch,embedding_size]\n",
    "        #pre_c = [batch,2*encoding_dim]\n",
    "        #pre_s = [batch,encoding_dim]\n",
    "        #The paper doesn't clearly describe how to merge pre_w and pre_c into one array passed to gru_cell, \n",
    "        #so that I use concatenation operator to combinate them.\n",
    "        gru_input = tf.concat([pre_w,pre_c],1)#[batch,embediing_size+2*encoding_dim]\n",
    "        gru_cell = tf.contrib.rnn.GRUCell(encoding_dim)\n",
    "        _,s_t = gru_cell(gru_input,pre_s)#[batch,encoding_dim]\n",
    "        \n",
    "        c_t = attention(pre_s,h_hat)#[batch,2*encoding_dim]\n",
    "#         r_t = tf.matmul(W_r,tf.transpose(pre_w))#[2*readout_size,batch]\n",
    "#         +tf.matmul(U_r,tf.transpose(pre_c))#[2*readout_size,batch]\n",
    "#         +tf.matmul(V_r,tf.transpose(pre_s))#[2*readout_size,batch]\n",
    "        r_t = tf.add(tf.add(tf.matmul(W_r,tf.transpose(pre_w)),tf.matmul(U_r,tf.transpose(pre_c))),tf.matmul(V_r,tf.transpose(pre_s)))\n",
    "        r_t = tf.reshape(r_t,[2,readout_size,batch_size])#[2,readout_size,batch]\n",
    "        m_t = tf.transpose(tf.reduce_max(r_t,0))#[batch,readout_size]\n",
    "        p_t = tf.matmul(m_t,W_o)#[batch,vocab_y_size]\n",
    "        #argmax_y = tf.argmax(p_t,1)#[batch]\n",
    "        #argmax_y = tf.nn.embedding_lookup(Y_embeddings,argmax_y)#[batch,embedding_size]\n",
    "        return c_t,s_t,p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def attention(pre_state,h_hat):\n",
    "    #h_hat=[batch,time,2*encoding_dim]\n",
    "    h_hat = tf.reshape(h_hat,[batch_size*content_length,2*encoding_dim])#[batch*time,2*encoding_dim]\n",
    "    h_hat = tf.transpose(h_hat)#[2*encoding_dim,batch*time]\n",
    "    #pre_state = [batch,encoding_dim]\n",
    "    WS = tf.expand_dims(tf.matmul(W_a,tf.transpose(pre_state)),2)#[embedding_size,batch,1]\n",
    "\n",
    "    UH = tf.reshape(tf.matmul(U_a,h_hat),[encoding_dim,batch_size,content_length])#[encoding_dim,batch,time]\n",
    "\n",
    "    e_ti = tf.tanh(WS+UH)#[encoding_dim,batch,time]\n",
    "  \n",
    "    e_ti = tf.reshape(tf.transpose(e_ti,[1,2,0]),[batch_size*content_length,encoding_dim])#[batch*time,encoding_dim]\n",
    "    e_ti = tf.matmul(e_ti,V_a)#[batch*time,1]\n",
    "    e_ti = tf.reshape(e_ti,[batch_size,content_length])#[batch,time]\n",
    "    a_ti = tf.nn.softmax(e_ti,1)#[batch,time]\n",
    "    a_ti = tf.expand_dims(a_ti,2)#[batch,time,1]\n",
    "    h_hat = tf.transpose(tf.reshape(h_hat,[2*encoding_dim,batch_size,content_length]),[1,0,2])#[batch,2*encoding_dim,time]\n",
    "    \n",
    "    c_t = tf.matmul(h_hat,a_ti)#[batch,2*encoding_dim,1]\n",
    "    \n",
    "    c_t = tf.squeeze(c_t)#[batch,2*encoding_dim]\n",
    "    return c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = tf.placeholder(tf.int32, [batch_size, content_length])\n",
    "trigger_Y = tf.placeholder(tf.int32, [batch_size,1])#[batch,1]\n",
    "\n",
    "Y_inputs_embedded = tf.nn.embedding_lookup(Y_embeddings, Y)#[batch,time,embedding_size]\n",
    "Y_trigger_inputs_embedding = tf.nn.embedding_lookup(Y_embeddings, trigger_Y)#[batch,1,embedding_size]\n",
    "#target = tf.placeholder(tf.int32,[batch_size,content_length,vocab_y_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decoder_training(h_hat,output_states):\n",
    "    with tf.variable_scope('decode') as scope:\n",
    "        pre_s = tf.tanh(tf.matmul(W_d,tf.transpose(output_states[1]))+S_b)#[encoding_dim,batch]\n",
    "        pre_s = tf.transpose(pre_s)#[batch,encoding_dim]\n",
    "        pre_c = attention(pre_s,h_hat)#[batch,2*encoding_dim]\n",
    "        time_major_Y = tf.unstack(tf.transpose(Y_inputs_embedded,[1,0,2]),axis=0)#[time,batch,embedding_size]\n",
    "        time_major_predict = []#[time,batch,vocab_y_size]\n",
    "        for step_,pre_w in enumerate(time_major_Y):\n",
    "            if step_ > 0:\n",
    "                scope.reuse_variables()\n",
    "            c_t,s_t,p_t = step(pre_w,pre_c,pre_s,h_hat)\n",
    "            time_major_predict.append(p_t)\n",
    "            pre_c = c_t\n",
    "            pre_s = s_t\n",
    "            #------------------when tesing, it must feed previous prediction Y into gru.\n",
    "            #argmax_y = tf.argmax(p_t,1)#[batch]\n",
    "            #argmax_y = tf.nn.embedding_lookup(Y_embeddings,argmax_y)#[batch,embedding_size]\n",
    "        return time_major_predict#[time,batch,vocab_y_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder_prediction(h_hat,output_states):\n",
    "    with tf.variable_scope('decode') as scope:\n",
    "        pre_s = tf.tanh(tf.matmul(W_d,tf.transpose(output_states[1]))+S_b)#[encoding_dim,batch]\n",
    "        pre_s = tf.transpose(pre_s)#[batch,encoding_dim]\n",
    "        pre_c = attention(pre_s,h_hat)#[batch,2*encoding_dim]\n",
    "        time_major_Y = tf.unstack(tf.transpose(Y_trigger_inputs_embedding,[1,0,2]),axis=0)#[1,batch,embedding_size]\n",
    "        time_major_predict = []#[time,batch,vocab_y_size]\n",
    "        pre_w = tf.squeeze(time_major_Y)\n",
    "        for step_ in range(content_length):\n",
    "            if step_ > 0:\n",
    "                scope.reuse_variables()\n",
    "            c_t,s_t,p_t = step(pre_w,pre_c,pre_s,h_hat)\n",
    "            time_major_predict.append(p_t)\n",
    "            pre_c = c_t\n",
    "            pre_s = s_t\n",
    "            #------------------when tesing, it must feed previous prediction Y into gru.\n",
    "            argmax_y = tf.argmax(p_t,1)#[batch]\n",
    "            next_embedding_y = tf.nn.embedding_lookup(Y_embeddings,argmax_y)#[batch,embedding_size]\n",
    "            pre_w = next_embedding_y\n",
    "        return time_major_predict#[time,batch,vocab_y_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_trigger_y_batch(batch_size_):\n",
    "    batch = np.zeros([batch_size_,1])#[batch,1]\n",
    "    batch.fill(EOS)#[batch]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train():  \n",
    "    h_hat,output_states = encoder()\n",
    "    time_major_predict = decoder_training(h_hat,output_states)\n",
    "    target = tf.placeholder(tf.int32,[batch_size,content_length])\n",
    "    target_one_hot = tf.one_hot(target,vocab_y_size,1,0)#[batch_size,content_length,vocab_y_size]\n",
    "    target_one_hot = tf.transpose(target_one_hot,[1,0,2])#[content_length,batch_size,vocab_y_size]\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=target_one_hot,logits=time_major_predict))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_grads_and_vars = [(tf.clip_by_value(g, -5.,5.), v) for g,v in grads_and_vars]\n",
    "    train_op = optimizer.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    " \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    " \n",
    "        # writer = tf.summary.FileWriter()\n",
    "        # 恢复前一次训练\n",
    "        ckpt = tf.train.get_checkpoint_state('.')\n",
    "        if ckpt != None:\n",
    "            print(ckpt.model_checkpoint_path)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print(\"没找到模型\")\n",
    " \n",
    "        for step_ in range(20000):\n",
    "            batch_train_x,batch_train_y,batch_target_y = data_gen.__next__()\n",
    "                \n",
    "            sess.run(train_op, feed_dict={X:batch_train_x, Y:batch_train_y, target:batch_target_y})\n",
    "            losses = sess.run(loss,feed_dict={X:batch_train_x, Y:batch_train_y, target:batch_target_y})\n",
    "            print(losses)\n",
    " \n",
    "            # 保存模型并计算准确率\n",
    "            if step_ % 1000 == 0:\n",
    "                path = saver.save(sess, './machine_reading.model', global_step=step_)\n",
    "                print(path)\n",
    " \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test():  \n",
    "    h_hat,output_states = encoder()\n",
    "    time_major_predict = decoder_prediction(h_hat,output_states)\n",
    "    target = tf.placeholder(tf.int32,[batch_size,content_length])\n",
    "    target_one_hot = tf.one_hot(target,vocab_y_size,1,0)#[batch_size,content_length,vocab_y_size]\n",
    "    target_one_hot = tf.transpose(target_one_hot,[1,0,2])#[content_length,batch_size,vocab_y_size]\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=target_one_hot,logits=time_major_predict))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_grads_and_vars = [(tf.clip_by_value(g, -5.,5.), v) for g,v in grads_and_vars]\n",
    "    train_op = optimizer.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    " \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    " \n",
    "        # writer = tf.summary.FileWriter()\n",
    "        # 恢复前一次训练\n",
    "        ckpt = tf.train.get_checkpoint_state('.')\n",
    "        if ckpt != None:\n",
    "            print(ckpt.model_checkpoint_path)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print(\"没找到模型\")\n",
    " \n",
    "        \n",
    "        batch_train_x,batch_train_y,batch_target_y = data_gen.__next__()\n",
    "        trigger_y_batch = get_trigger_y_batch(batch_size)#[batch,1]\n",
    "        \n",
    "        time_major_predict = sess.run(time_major_predict,\n",
    "                 feed_dict={X:batch_train_x, \n",
    "                            Y:batch_train_y, \n",
    "                            target:batch_target_y,\n",
    "                            trigger_Y:trigger_y_batch})#[time,batch,vocab_y_size]\n",
    "        \n",
    "        time_major_predict = np.stack(time_major_predict)#[time,batch,vocab_y_size]\n",
    "        time_major_predict = tf.argmax(time_major_predict,2)#[time,batch]\n",
    "        print(time_major_predict.shape)\n",
    "        time_major_predict = sess.run(time_major_predict)#tensor to ndarray\n",
    "        batch_major_predict = np.transpose(time_major_predict,[1,0])#[batch,time]\n",
    "        print(batch_major_predict.shape)\n",
    "        #batch_major_predict = np.squeeze(batch_major_predict)#[batch,time]\n",
    "        for row in np.nditer(batch_major_predict,flags=['external_loop'],order='F'):\n",
    "            #row = np.squeeze(row)#[time]\n",
    "            words = list(map(lambda x:idx2w_y[x],row))\n",
    "            print(''.join(words))\n",
    "        #losses = sess.run(loss,feed_dict={X:batch_train_x, Y:batch_train_y, target:batch_target_y})\n",
    "        #print(losses)\n",
    " \n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "没找到模型\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1024,100,100]\n\t [[Node: gradients/decode/step_1/transpose_3_grad/transpose = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/decode/step_1/MatMul_3_grad/tuple/control_dependency, gradients/decode/step_1/transpose_3_grad/InvertPermutation)]]\n\nCaused by op 'gradients/decode/step_1/transpose_3_grad/transpose', defined at:\n  File \"D:\\Anaconda3\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"D:\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 653, in launch_instance\n    app.start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-93fd337a0d5c>\", line 1, in <module>\n    train()\n  File \"<ipython-input-19-97e354078730>\", line 10, in train\n    grads_and_vars = optimizer.compute_gradients(loss)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 386, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 562, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 368, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 562, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py\", line 427, in _TransposeGrad\n    return [array_ops.transpose(grad, array_ops.invert_permutation(p)), None]\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1272, in transpose\n    ret = gen_array_ops.transpose(a, perm, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3749, in transpose\n    result = _op_def_lib.apply_op(\"Transpose\", x=x, perm=perm, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'decode/step_1/transpose_3', defined at:\n  File \"D:\\Anaconda3\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 19 identical lines from previous traceback]\n  File \"<ipython-input-21-93fd337a0d5c>\", line 1, in <module>\n    train()\n  File \"<ipython-input-19-97e354078730>\", line 3, in train\n    time_major_predict = decoder_training(h_hat,output_states)\n  File \"<ipython-input-16-c3364ff8bbda>\", line 11, in decoder_training\n    c_t,s_t,p_t = step(pre_w,pre_c,pre_s,h_hat)\n  File \"<ipython-input-13-9e811637e8fe>\", line 12, in step\n    c_t = attention(pre_s,h_hat)#[batch,2*encoding_dim]\n  File \"<ipython-input-14-90d959fc8ab8>\", line 17, in attention\n    h_hat = tf.transpose(tf.reshape(h_hat,[2*encoding_dim,batch_size,content_length]),[1,0,2])#[batch,2*encoding_dim,time]\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1272, in transpose\n    ret = gen_array_ops.transpose(a, perm, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3749, in transpose\n    result = _op_def_lib.apply_op(\"Transpose\", x=x, perm=perm, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1024,100,100]\n\t [[Node: gradients/decode/step_1/transpose_3_grad/transpose = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/decode/step_1/MatMul_3_grad/tuple/control_dependency, gradients/decode/step_1/transpose_3_grad/InvertPermutation)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1032\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024,100,100]\n\t [[Node: gradients/decode/step_1/transpose_3_grad/transpose = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/decode/step_1/MatMul_3_grad/tuple/control_dependency, gradients/decode/step_1/transpose_3_grad/InvertPermutation)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-93fd337a0d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-97e354078730>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mbatch_train_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_train_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_target_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_train_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_train_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_target_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_train_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_train_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_target_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 786\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    787\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 994\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    995\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1044\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1045\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024,100,100]\n\t [[Node: gradients/decode/step_1/transpose_3_grad/transpose = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/decode/step_1/MatMul_3_grad/tuple/control_dependency, gradients/decode/step_1/transpose_3_grad/InvertPermutation)]]\n\nCaused by op 'gradients/decode/step_1/transpose_3_grad/transpose', defined at:\n  File \"D:\\Anaconda3\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"D:\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 653, in launch_instance\n    app.start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-93fd337a0d5c>\", line 1, in <module>\n    train()\n  File \"<ipython-input-19-97e354078730>\", line 10, in train\n    grads_and_vars = optimizer.compute_gradients(loss)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 386, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 562, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 368, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 562, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py\", line 427, in _TransposeGrad\n    return [array_ops.transpose(grad, array_ops.invert_permutation(p)), None]\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1272, in transpose\n    ret = gen_array_ops.transpose(a, perm, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3749, in transpose\n    result = _op_def_lib.apply_op(\"Transpose\", x=x, perm=perm, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'decode/step_1/transpose_3', defined at:\n  File \"D:\\Anaconda3\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 19 identical lines from previous traceback]\n  File \"<ipython-input-21-93fd337a0d5c>\", line 1, in <module>\n    train()\n  File \"<ipython-input-19-97e354078730>\", line 3, in train\n    time_major_predict = decoder_training(h_hat,output_states)\n  File \"<ipython-input-16-c3364ff8bbda>\", line 11, in decoder_training\n    c_t,s_t,p_t = step(pre_w,pre_c,pre_s,h_hat)\n  File \"<ipython-input-13-9e811637e8fe>\", line 12, in step\n    c_t = attention(pre_s,h_hat)#[batch,2*encoding_dim]\n  File \"<ipython-input-14-90d959fc8ab8>\", line 17, in attention\n    h_hat = tf.transpose(tf.reshape(h_hat,[2*encoding_dim,batch_size,content_length]),[1,0,2])#[batch,2*encoding_dim,time]\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1272, in transpose\n    ret = gen_array_ops.transpose(a, perm, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3749, in transpose\n    result = _op_def_lib.apply_op(\"Transpose\", x=x, perm=perm, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1024,100,100]\n\t [[Node: gradients/decode/step_1/transpose_3_grad/transpose = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/decode/step_1/MatMul_3_grad/tuple/control_dependency, gradients/decode/step_1/transpose_3_grad/InvertPermutation)]]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
