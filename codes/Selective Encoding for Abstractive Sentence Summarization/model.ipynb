{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"toy_data.txt\",'r',encoding='utf-8')as fn:\n",
    "    lines = fn.readlines()\n",
    "#     train = lines[:200000]\n",
    "#     dev = lines[200000:400000]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = lines[1::2]\n",
    "train_y = lines[0::2]\n",
    "train_x = list(map(lambda x:x.replace(\"\\n\",\"\").strip(),train_x))\n",
    "train_y = list(map(lambda x:x.replace(\"\\n\",\"\").strip(),train_y))\n",
    "\n",
    "# dev_x = dev[1::2]\n",
    "# dev_y = dev[0::2]\n",
    "# dev_x = list(map(lambda x:x.replace(\"\\n\",\"\").strip(),dev_x))\n",
    "# dev_y = list(map(lambda x:x.replace(\"\\n\",\"\").strip(),dev_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "創建x和y的vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_x = set()\n",
    "vocab_y = set()\n",
    "train_x = list(map(lambda x:list(x),train_x))\n",
    "train_y = list(map(lambda x:list(x),train_y))\n",
    "# dev_x = list(map(lambda x:list(x),dev_x))\n",
    "# dev_y = list(map(lambda x:list(x),dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for s in train_x:\n",
    "    vocab_x.update(s)\n",
    "for s in train_y:\n",
    "    vocab_y.update(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('vocab_x','w',encoding='utf-8')as fno:\n",
    "    for w in vocab_x:\n",
    "        fno.write(w+\"\\n\")\n",
    "with open('vocab_y','w',encoding='utf-8')as fno:\n",
    "    for w in vocab_y:\n",
    "        fno.write(w+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2idx_x = dict()\n",
    "w2idx_y = dict()\n",
    "with open('vocab_x','r',encoding='utf-8')as fn:\n",
    "    lines = fn.readlines()\n",
    "    for idx,w in enumerate(lines):\n",
    "        w2idx_x[w[:-1]] = idx+2\n",
    "with open('vocab_y','r',encoding='utf-8')as fn:\n",
    "    lines = fn.readlines()\n",
    "    for idx,w in enumerate(lines):\n",
    "        w2idx_y[w[:-1]] = idx+2\n",
    "idx2w_x = dict()\n",
    "idx2w_y = dict()\n",
    "for w, idx in w2idx_x.items():\n",
    "    idx2w_x[idx] = w\n",
    "for w, idx in w2idx_y.items():\n",
    "    idx2w_y[idx] = w\n",
    "idx2w_x[0] = \"PAD\"\n",
    "idx2w_x[1] = \"EOS\"\n",
    "idx2w_y[0] = \"PAD\"\n",
    "idx2w_y[1] = \"EOS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x_temp = list(map(lambda x:list(map(lambda y:w2idx_x[y],x)),train_x))\n",
    "train_y_temp = list(map(lambda x:list(map(lambda y:w2idx_y[y],x)),train_y))\n",
    "with open('train_x_vec','w',encoding='utf-8')as fno:\n",
    "    for s in train_x_temp:\n",
    "        fno.write(str(s)+'\\n')\n",
    "with open('train_y_vec','w',encoding='utf-8')as fno:\n",
    "    for s in train_y_temp:\n",
    "        fno.write(str(s)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import shift\n",
    "from keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(data_file):\n",
    "    stories = []\n",
    "    with open(data_file,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = ast.literal_eval(line.strip())\n",
    "            stories.append(line)\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = read_data('train_x_vec')\n",
    "train_y = read_data('train_y_vec')\n",
    "target_y = list(map(lambda x: x+[EOS],train_y))\n",
    "train_y = list(map(lambda x: [EOS]+x,train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = sequence.pad_sequences(train_x, maxlen=100, dtype='int32',padding='post', truncating='post', value=0.)\n",
    "train_y = sequence.pad_sequences(train_y, maxlen=100, dtype='int32',padding='post', truncating='post', value=0.)\n",
    "target_y = sequence.pad_sequences(target_y, maxlen=100, dtype='int32',padding='post', truncating='post', value=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開始建構graph和training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "content_length=100\n",
    "encoding_dim = 512\n",
    "vocab_x_size = 2882+2\n",
    "vocab_y_size = 1416+2\n",
    "embedding_size = 100\n",
    "readout_size = embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_next_batch():\n",
    "    cur = 0\n",
    "    pre = 0\n",
    "    while(True):\n",
    "        cur = cur+batch_size\n",
    "        yield train_x[pre:cur],train_y[pre:cur],target_y[pre:cur]\n",
    "        pre = cur\n",
    "        cur = cur%448\n",
    "        pre = pre%448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_gen = get_next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X = tf.placeholder(tf.int32, [batch_size, content_length])\n",
    "\n",
    "X_embeddings = tf.Variable(tf.random_uniform([vocab_x_size, embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "Y_embeddings = tf.Variable(tf.random_uniform([vocab_y_size, embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "X_inputs_embedded = tf.nn.embedding_lookup(X_embeddings, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_s = tf.Variable(tf.random_uniform([2*encoding_dim, 2*encoding_dim], -1.0, 1.0), dtype=tf.float32)\n",
    "W_b = tf.Variable(tf.random_uniform([2*encoding_dim,1,1], -1.0, 1.0), dtype=tf.float32)\n",
    "U_s = tf.Variable(tf.random_uniform([2*encoding_dim, 2*encoding_dim], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "def encoder():\n",
    "    with tf.variable_scope('encode') as scope:\n",
    "        X_lens = tf.reduce_sum(tf.sign(tf.abs(X)), 1)\n",
    "        gru_cell_fw = tf.contrib.rnn.GRUCell(encoding_dim)\n",
    "        gru_cell_bw = tf.contrib.rnn.GRUCell(encoding_dim)\n",
    "        outputs, output_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                        gru_cell_fw,\n",
    "                                        gru_cell_bw,\n",
    "                                        X_inputs_embedded,\n",
    "                                        sequence_length=X_lens,\n",
    "                                        dtype=tf.float32)\n",
    "        h = tf.concat(outputs,2)#[batch,time,2*encoding_dim]\n",
    "        h = tf.reshape(h,[batch_size*content_length,2*encoding_dim])#[batch*time,2*encoding_dim]\n",
    "        s = tf.concat(output_states,1)#[?,2*encoding_dim]\n",
    "        WH = tf.matmul(W_s,tf.transpose(h))#[2*encoding_dim,batch*time]\n",
    "        \n",
    "        WH = tf.reshape(WH,[2*encoding_dim,batch_size,content_length])#[2*encoding_dim,batch,time]\n",
    "        US = tf.matmul(U_s,tf.transpose(s))#[2*encoding_dim,?]\n",
    "        US = tf.expand_dims(US,2)#[2*encoding_dim,?,1]\n",
    "        \n",
    "        sGate = tf.sigmoid(WH+US+W_b)#[2*encoding_dim,batch,time]\n",
    "        \n",
    "        h = tf.reshape(h,[batch_size,content_length,2*encoding_dim])#[batch,time,2*encoding_dim]\n",
    "\n",
    "        sGate = tf.transpose(sGate,[1,2,0])#[batch,time,2*encoding_dim]\n",
    "        h_hat = h*sGate#[batch,time,2*encoding_dim]\n",
    "        \n",
    "        return h_hat,output_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_d = tf.Variable(tf.random_uniform([encoding_dim, encoding_dim], -1.0, 1.0), dtype=tf.float32)\n",
    "S_b = tf.Variable(tf.random_uniform([encoding_dim, 1], -1.0, 1.0), dtype=tf.float32)\n",
    "W_a = tf.Variable(tf.random_uniform([encoding_dim, encoding_dim], -1.0, 1.0), dtype=tf.float32)\n",
    "U_a = tf.Variable(tf.random_uniform([encoding_dim, 2*encoding_dim], -1.0, 1.0), dtype=tf.float32)\n",
    "V_a = tf.Variable(tf.random_uniform([encoding_dim, 1], -1.0, 1.0), dtype=tf.float32)\n",
    "W_r = tf.Variable(tf.random_uniform([2*readout_size, embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "U_r = tf.Variable(tf.random_uniform([2*readout_size, 2*encoding_dim], -1.0, 1.0), dtype=tf.float32)\n",
    "V_r = tf.Variable(tf.random_uniform([2*readout_size, encoding_dim], -1.0, 1.0), dtype=tf.float32)\n",
    "W_o = tf.Variable(tf.random_uniform([readout_size, vocab_y_size], -1.0, 1.0), dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def step(pre_w,pre_c,pre_s,h_hat):\n",
    "    with tf.variable_scope('step') as scope:\n",
    "        #pre_w = [batch,embedding_size]\n",
    "        #pre_c = [batch,2*encoding_dim]\n",
    "        #pre_s = [batch,encoding_dim]\n",
    "        #The paper doesn't clearly describe how to merge pre_w and pre_c into one array passed to gru_cell, \n",
    "        #so that I use concatenation operator to combinate them.\n",
    "        gru_input = tf.concat([pre_w,pre_c],1)#[batch,embediing_size+2*encoding_dim]\n",
    "        gru_cell = tf.contrib.rnn.GRUCell(encoding_dim)\n",
    "        _,s_t = gru_cell(gru_input,pre_s)#[batch,encoding_dim]\n",
    "        \n",
    "        c_t = attention(pre_s,h_hat)#[batch,2*encoding_dim]\n",
    "#         r_t = tf.matmul(W_r,tf.transpose(pre_w))#[2*readout_size,batch]\n",
    "#         +tf.matmul(U_r,tf.transpose(pre_c))#[2*readout_size,batch]\n",
    "#         +tf.matmul(V_r,tf.transpose(pre_s))#[2*readout_size,batch]\n",
    "        r_t = tf.add(tf.add(tf.matmul(W_r,tf.transpose(pre_w)),tf.matmul(U_r,tf.transpose(pre_c))),tf.matmul(V_r,tf.transpose(pre_s)))\n",
    "        r_t = tf.reshape(r_t,[2,readout_size,batch_size])#[2,readout_size,batch]\n",
    "        m_t = tf.transpose(tf.reduce_max(r_t,0))#[batch,readout_size]\n",
    "        p_t = tf.matmul(m_t,W_o)#[batch,vocab_y_size]\n",
    "        #argmax_y = tf.argmax(p_t,1)#[batch]\n",
    "        #argmax_y = tf.nn.embedding_lookup(Y_embeddings,argmax_y)#[batch,embedding_size]\n",
    "        return c_t,s_t,p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def attention(pre_state,h_hat):\n",
    "    #h_hat=[batch,time,2*encoding_dim]\n",
    "    h_hat = tf.reshape(h_hat,[batch_size*content_length,2*encoding_dim])#[batch*time,2*encoding_dim]\n",
    "    h_hat = tf.transpose(h_hat)#[2*encoding_dim,batch*time]\n",
    "    #pre_state = [batch,encoding_dim]\n",
    "    WS = tf.expand_dims(tf.matmul(W_a,tf.transpose(pre_state)),2)#[embedding_size,batch,1]\n",
    "\n",
    "    UH = tf.reshape(tf.matmul(U_a,h_hat),[encoding_dim,batch_size,content_length])#[encoding_dim,batch,time]\n",
    "\n",
    "    e_ti = tf.tanh(WS+UH)#[encoding_dim,batch,time]\n",
    "  \n",
    "    e_ti = tf.reshape(tf.transpose(e_ti,[1,2,0]),[batch_size*content_length,encoding_dim])#[batch*time,encoding_dim]\n",
    "    e_ti = tf.matmul(e_ti,V_a)#[batch*time,1]\n",
    "    e_ti = tf.reshape(e_ti,[batch_size,content_length])#[batch,time]\n",
    "    a_ti = tf.nn.softmax(e_ti,1)#[batch,time]\n",
    "    a_ti = tf.expand_dims(a_ti,2)#[batch,time,1]\n",
    "    h_hat = tf.transpose(tf.reshape(h_hat,[2*encoding_dim,batch_size,content_length]),[1,0,2])#[batch,2*encoding_dim,time]\n",
    "    \n",
    "    c_t = tf.matmul(h_hat,a_ti)#[batch,2*encoding_dim,1]\n",
    "    \n",
    "    c_t = tf.squeeze(c_t)#[batch,2*encoding_dim]\n",
    "    return c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = tf.placeholder(tf.int32, [batch_size, content_length])\n",
    "trigger_Y = tf.placeholder(tf.int32, [batch_size,1])#[batch,1]\n",
    "\n",
    "Y_inputs_embedded = tf.nn.embedding_lookup(Y_embeddings, Y)#[batch,time,embedding_size]\n",
    "Y_trigger_inputs_embedding = tf.nn.embedding_lookup(Y_embeddings, trigger_Y)#[batch,1,embedding_size]\n",
    "#target = tf.placeholder(tf.int32,[batch_size,content_length,vocab_y_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decoder_training(h_hat,output_states):\n",
    "    with tf.variable_scope('decode') as scope:\n",
    "        pre_s = tf.tanh(tf.matmul(W_d,tf.transpose(output_states[1]))+S_b)#[encoding_dim,batch]\n",
    "        pre_s = tf.transpose(pre_s)#[batch,encoding_dim]\n",
    "        pre_c = attention(pre_s,h_hat)#[batch,2*encoding_dim]\n",
    "        time_major_Y = tf.unstack(tf.transpose(Y_inputs_embedded,[1,0,2]),axis=0)#[time,batch,embedding_size]\n",
    "        time_major_predict = []#[time,batch,vocab_y_size]\n",
    "        for step_,pre_w in enumerate(time_major_Y):\n",
    "            if step_ > 0:\n",
    "                scope.reuse_variables()\n",
    "            c_t,s_t,p_t = step(pre_w,pre_c,pre_s,h_hat)\n",
    "            time_major_predict.append(p_t)\n",
    "            pre_c = c_t\n",
    "            pre_s = s_t\n",
    "            #------------------when tesing, it must feed previous prediction Y into gru.\n",
    "            #argmax_y = tf.argmax(p_t,1)#[batch]\n",
    "            #argmax_y = tf.nn.embedding_lookup(Y_embeddings,argmax_y)#[batch,embedding_size]\n",
    "        return time_major_predict#[time,batch,vocab_y_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder_prediction(h_hat,output_states):\n",
    "    with tf.variable_scope('decode') as scope:\n",
    "        pre_s = tf.tanh(tf.matmul(W_d,tf.transpose(output_states[1]))+S_b)#[encoding_dim,batch]\n",
    "        pre_s = tf.transpose(pre_s)#[batch,encoding_dim]\n",
    "        pre_c = attention(pre_s,h_hat)#[batch,2*encoding_dim]\n",
    "        time_major_Y = tf.unstack(tf.transpose(Y_trigger_inputs_embedding,[1,0,2]),axis=0)#[1,batch,embedding_size]\n",
    "        time_major_predict = []#[time,batch,vocab_y_size]\n",
    "        pre_w = tf.squeeze(time_major_Y)\n",
    "        for step_ in range(content_length):\n",
    "            if step_ > 0:\n",
    "                scope.reuse_variables()\n",
    "            c_t,s_t,p_t = step(pre_w,pre_c,pre_s,h_hat)\n",
    "            time_major_predict.append(p_t)\n",
    "            pre_c = c_t\n",
    "            pre_s = s_t\n",
    "            #------------------when tesing, it must feed previous prediction Y into gru.\n",
    "            argmax_y = tf.argmax(p_t,1)#[batch]\n",
    "            next_embedding_y = tf.nn.embedding_lookup(Y_embeddings,argmax_y)#[batch,embedding_size]\n",
    "            pre_w = next_embedding_y\n",
    "        return time_major_predict#[time,batch,vocab_y_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_trigger_y_batch(batch_size_):\n",
    "    batch = np.zeros([batch_size_,1])#[batch,1]\n",
    "    batch.fill(EOS)#[batch]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train():  \n",
    "    h_hat,output_states = encoder()\n",
    "    time_major_predict = decoder_training(h_hat,output_states)\n",
    "    target = tf.placeholder(tf.int32,[batch_size,content_length])\n",
    "    target_one_hot = tf.one_hot(target,vocab_y_size,1,0)#[batch_size,content_length,vocab_y_size]\n",
    "    target_one_hot = tf.transpose(target_one_hot,[1,0,2])#[content_length,batch_size,vocab_y_size]\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=target_one_hot,logits=time_major_predict))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_grads_and_vars = [(tf.clip_by_value(g, -5.,5.), v) for g,v in grads_and_vars]\n",
    "    train_op = optimizer.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    " \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    " \n",
    "        # writer = tf.summary.FileWriter()\n",
    "        # 恢复前一次训练\n",
    "        ckpt = tf.train.get_checkpoint_state('.')\n",
    "        if ckpt != None:\n",
    "            print(ckpt.model_checkpoint_path)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print(\"没找到模型\")\n",
    " \n",
    "        for step_ in range(20000):\n",
    "            batch_train_x,batch_train_y,batch_target_y = data_gen.__next__()\n",
    "                \n",
    "            sess.run(train_op, feed_dict={X:batch_train_x, Y:batch_train_y, target:batch_target_y})\n",
    "            losses = sess.run(loss,feed_dict={X:batch_train_x, Y:batch_train_y, target:batch_target_y})\n",
    "            print(losses)\n",
    " \n",
    "            # 保存模型并计算准确率\n",
    "            if step_ % 1000 == 0:\n",
    "                path = saver.save(sess, './machine_reading.model', global_step=step_)\n",
    "                print(path)\n",
    " \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test():  \n",
    "    h_hat,output_states = encoder()\n",
    "    time_major_predict = decoder_prediction(h_hat,output_states)\n",
    "    target = tf.placeholder(tf.int32,[batch_size,content_length])\n",
    "    target_one_hot = tf.one_hot(target,vocab_y_size,1,0)#[batch_size,content_length,vocab_y_size]\n",
    "    target_one_hot = tf.transpose(target_one_hot,[1,0,2])#[content_length,batch_size,vocab_y_size]\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=target_one_hot,logits=time_major_predict))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_grads_and_vars = [(tf.clip_by_value(g, -5.,5.), v) for g,v in grads_and_vars]\n",
    "    train_op = optimizer.apply_gradients(capped_grads_and_vars)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    " \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    " \n",
    "        # writer = tf.summary.FileWriter()\n",
    "        # 恢复前一次训练\n",
    "        ckpt = tf.train.get_checkpoint_state('.')\n",
    "        if ckpt != None:\n",
    "            print(ckpt.model_checkpoint_path)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print(\"没找到模型\")\n",
    " \n",
    "        \n",
    "        batch_train_x,batch_train_y,batch_target_y = data_gen.__next__()\n",
    "        trigger_y_batch = get_trigger_y_batch(batch_size)#[batch,1]\n",
    "        \n",
    "        time_major_predict = sess.run(time_major_predict,\n",
    "                 feed_dict={X:batch_train_x, \n",
    "                            Y:batch_train_y, \n",
    "                            target:batch_target_y,\n",
    "                            trigger_Y:trigger_y_batch})#[time,batch,vocab_y_size]\n",
    "        \n",
    "        time_major_predict = np.stack(time_major_predict)#[time,batch,vocab_y_size]\n",
    "        time_major_predict = tf.argmax(time_major_predict,2)#[time,batch]\n",
    "        print(time_major_predict.shape)\n",
    "        time_major_predict = sess.run(time_major_predict)#tensor to ndarray\n",
    "        batch_major_predict = np.transpose(time_major_predict,[1,0])#[batch,time]\n",
    "        print(batch_major_predict.shape)\n",
    "        #batch_major_predict = np.squeeze(batch_major_predict)#[batch,time]\n",
    "        for row in np.nditer(batch_major_predict,flags=['external_loop'],order='F'):\n",
    "            #row = np.squeeze(row)#[time]\n",
    "            words = list(map(lambda x:idx2w_y[x],row))\n",
    "            print(''.join(words))\n",
    "        #losses = sess.run(loss,feed_dict={X:batch_train_x, Y:batch_train_y, target:batch_target_y})\n",
    "        #print(losses)\n",
    " \n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\machine_reading.model-1000\n",
      "INFO:tensorflow:Restoring parameters from .\\machine_reading.model-1000\n",
      "(100, 64)\n",
      "(64, 100)\n",
      "日手G旗大持英英全調華亞相亞觀緊傳樂日三中宏英深F分三思加南美陸巴美鴻諾A停樂英福觀福英i蘋物C雲企D達T蘋抓指騰非陸美C創電3本機o下陸續政特球研為太中馬察跟南金亞星國達特港e析星科州非國電西國海基T損金特特察特特P果聯o端業a美o果住紋訊洲1國h業動D再遊o易通拓府爾創：在電線遜：台韓2化電郵電爾通d：再傳再礦路動總副、亞＆賣電爾：：、爾h取網g服財r航y、大辨與行0大e如腳列興戲g華富展批：新S美信上收英積IQY子政推發實官英發近度工易車統總騰證T單子展2V百、o代商n務務k空o三陸識C動月型i馬踏印策吸l電微電准將指o國大遊購特電l'A副宣展表施員特邀期爆齊斯廠D統訊實班出手示0e度AnI機i新管t全t星民新M服起銀l拉車新略引e、電氣建讓數n首手戲遊爾步j1G會布新J方預爾請擬發聚安威iJ領將表籠機無2l1ReB廣t創理r球a9眾創C務允行 松風創2玩推長停解造新排y次筆商戲晶伐i6專長與硬o案告押函裁森魯那馬lo投於露　晶線1o.M庫M布i：妙a大發月對：聯需許採W　靡：0家出華牌決全的名手推自機社圓　n手利權滴體u獲最寶　員林斯州完me　4餡美片V年d5大存成　vO招c當展的日L姻求貨撙o解大F1　D科交方球U出機出建　群代K取機訴五滴頻l批快MG1大登史成a 印Q　元轉R針y億和呆為市er　e機人「本i　大車節r決陸o6騰u將易案最S爐A榮網群公工L得部訟鉉出繁e　9Re.火堡無A B即重i兌單頭對n美解料企場Sb小網　工腕商q新增在措l問　r以訊o在　　大B　p耀路創司結AT門　　行　晶大月盼a4EOS悼前輪Ri時返P日英盔叫e元　風業拓ci家路暴智」品u公　4施d題美mI第 台傳伊離 大p品　將C盟-e裁在今建今片陸便重r萬於念例融od通手h圓特P車為投獲險採展at庭安露慧力熱i司惟省　w與國lo2A上醞頓岸T陸當牌採量uATs減德年立年模實可返 人大馬的資ue訊機o貶爾r服何資授大購需le也全航　之愛d有商城用i成卻aT季p市釀看風y首機手購產rRel4國上戰下組現能榮S　火里洪　sn新、n破　o務受V權　數先er受防空成爭不研望機市不d功不b、營p櫃部好力p度率機愛全sMna.逆半略半　人再景3佔EOS卡災籌s走創平e1高j推百e代台位確開a用護、日可減發打與銷起e決買sA收　　分儲發e進低EOS立球e　c電2轉薪合多支民度　智員EOS納已得e訪業板 0通e全度l工系硬認發讓EOS系銀本期　指破風售I總心單立I、主董入能電-榜　EOS信首　加o動%勝資作款援幣升晶慧工提大造1f科者市70不c自與oA蘋體定企企　統行製EOS 紋線險　B裁為EOS體、獲打座股市廠C前摩EOS設款欲劇r車員億新EOS新R加息圓手總網屠成0f索H場傳大妙t駕福dR果首位業業額　業造進豌辨上並製M：必EOS光機利基黃艾場EOS成2托EOS備1強全針電工光台EOS品e入EOS代表數穩殺1億在沃iEOS9關EOS 車特yM供選及雲採季以者業缺豆識音存造 零要相固器雙本嘉克未進為5羅PAD22化球對池EOSEOS幣EOS出aSEOS工可2EOSEOS0美F討k能月EOS高AEOS青n手應EOS商端購器人I轉之公支樂EOS商W售條習化人雙視能爾來PAD最強拉PAD80旗11、EOS此8EOS貨lDEOS策望0EOS網死元a論e針2EOSEOSl自睞e機鏈債業認軟P體T型EOS主付市企瞄a業件際列為優訊：EOS發PAD佳EOS與PAD億H下00電PAD鵬,PADEOSSR不略月%提深EOSEOSc巴獲遭3EOS高l相？　晶硬k模知體力免系象力購系場寄準t技EOS絕印關於通可自展PAD音EOS宏PAD元zT奈、解PAD罪0PAD能e部齡轉底EOS爾N於PADe爾1市日相EOSoEOS人替片起討式運前如疫統徵EOS物統的性雅s術網EOS技鍵預話跟填EOSPAD訊EOS達PADEOS高w米7銅PAD復0PAD大n分再彎於EOS等狂讓PADb幹.跨出習EOSyEOS才自EOS來訪EOS算可EOS系老EOS力AEOS既學痞o創監能術　期服大析就PAD連相電PAD建色i製奈箔PAD健0PAD陸s承EOS求IEOS諧EOSEOSPADo半7彙貨超不EOSEOS是駕EOSEOS驗諧產先EOS統舊進EOSp進有復族n新據相　聚EOS務陸算電PAD接習整PAD奈飽t程米供PAD用萬PAD樂e諾不財FEOS今EOSEOSPADo島5發EOS然通EOSEOS主技pEOSs展品在網為隱止力p確格礦群EOS首持習成焦EOSEOS「EOSEOSPAD埠推體PAD車Nc競推應PAD甚元PAD救視EOS升EOSAEOSB此進PADk緊億平　通姻EOSEOS因術線網債　EOS雲路概憂EOSPAD異O局EOS的浮重駕豌品自EOS　平PAD網PADEOSEOS表PADEOSBh賽光商PAD景EOSPAD兌覺　國於亮EOSI界員PAD直張美板PAD耳EOSEOSEOSEOS鋪Nik模力端EOS念EOS於PAD軍一EOSPAD輕帶滿題i更動EOS商等PAD魅PADEOS進現PAD網面競EOS罩資PAD僅於PAD妝應加EOSEOS相EOSEOS暴發PAD播關元hPADEOSEOSEOSEOSEOS路A他討商O測EOSEOSEOS書PAD突TPADPAD卡光足　理精駕EOS虹」PAD未PADEOS徒最PAD死板爭防檢格PAD儲8PAD貶用約EOS於EOSEOSEOSEOS係PAD朗係EOS兒PAD8不EOSEOS網EOS算泰訪店發試PAD網、7PAD起系PADPAD車證顧籲習細駛EOS奈談PAD來PADEOSA佳PAD體EOS力工測EOSPAD池於PAD爾EOSDEOSNEOS鋼提PADEOSPAD讀EOSPAD合PAD　通書EOS強都EOS薩驗3西EOSPAD作者提PADEOSPADPADPAD市旬客據助EOS、EOSd合PAD盟PADEOS地EOSPAD跟高EOS觸系EOSPAD運真PAD坐EOS批不奈EOS效網PADEOSPAD公　PADPADPAD美姻PEOSr音豌來s遭延於PAD改外盪PAD網PADPADPAD場EOS需程EOSEOS健構EOS作PAD準PADEOSL驅PAD商EOS力庭統得PADL驅PADEOSEOS鬨齡偏EOS效魅PAD進PAD開架PADPADPAD國還EOSEOS日高線來債全知讓PAD盪7朝PAD書PADPADPADEOSEOS求為EOSEOS康四PADEOSPADEOSPADEOSEOSEOSPAD先網線斷EOS到PAD幹8PAD進EOS相EOS策網駕DPAD員PAD信半PADPADPAD碗EOSEOSEOSr機Nc逐球運緒PAD季局EOSPADEOSPADPADPADPAD此EOS抗EOSPAD醫奇PAD此PAD豌PAD線EOSPADPAD化書EOS聯　EOSPAD俄幫PADEOSEOS增不達彙演rPAD長PADEOS6PADPADPAD附不EOSEOS日技AU上EOS閉啟PAD強跳於PAD進PADPADPADPAD遭EOSPAD不PAD療營PAD遭PADoPAD線EOSPADPADEOSEOSEOS米亂於PAD　新PADEOSEOS權習EOS訟績聯PAD習PAD芝半PADPADPAD造通EOS相r元算運逐諧EOS到PAD微蝕書PAD邏PADPADPADPADPAD網功EOSPAD與算PAD試PAD市PADxEOSPADPAD於高EOS擁力百PAD驅晶PAD豌EOS上助能r爭EOSPAD潮PAD層籌PADPADPADEOS拉EOS習日小EOS晶上展PAD盡PAD、受7PAD溫PADPADPADPADPAD斯成PADPAD智唯PAD芝PAD場PAD部EOSPADPAD8榜PAD：繁供PAD國EOSPAD萬鋼果EOS所PAD演DPAD透PAD真居PADPADPADEOS育書助rEOS諧EOS逐確PAD試PADg局提PADAPADPADPADPADPAD美PADPADPAD慧像PAD0PAD控PAD回相PADPAD億產PAD步誓力PAD7於PAD品專＆EOSPADPAD造rPADEOSPADFEOSPADPADPADEOS零強服是高碩亂上認PAD朗PAD提跳EOSPADEOSPADPADPADPADPAD技PADPADPAD工義PAD碼PAD證PAD妨習PADPAD美煞PAD晶維EOSPAD可8PADEOS乾韓EOSPADPAD暴材PADEOSPAD繁為PADPADPADEOS透i服與機A1逐陳PAD輯PAD作蝕業PAD進PADPADPADPADPAD元PADPADPAD廠家PAD財PAD主PAD7推PADPAD元PADPAD渺猶此PAD於達PAD熱力　不PADPADh應PAD進PAD事常PADPADPAD相EOS效服步技R他上也PAD硬PAD　受亮PAD不PADPADPADPADPADEOSPADPADPAD　伏PADEOSPAD未PAD運EOSPADPADEOSPADPADEOS對炒PAD監熱PAD板欲加習PADPAD訓家PAD發PAD信嶸PADPADPAD習高官服存術紅n逐動PAD藉PAD以局EOSPAD減PADPADPADPADPAD網PADPADPAD推訊PAD驅PAD來PAD債進PADPADEOSPADPAD7之崛PAD、專PADeE老助PADPADEOS成PAD機PADEOS甚PADPADPAD超EOSc服陸鋪確輪上蕊PAD疑PAD頻錯EOSPADEOSPADPADPADPADPAD斯PADPADPAD升機PAD8PAD返PAD車徒PADPADEOSPADPAD工EOS保PAD碳案PAD造檻和EOSPADPADEOS德PAD燃PAD豌擊PADPADPAD效高傳態吹EOS賈減逐啟PAD則PAD為遏書PAD不PADPADPADPADPAD五PADPADPAD2往PAD許PAD中PAD顧APADPAD網PADPAD僅此盈PAD替過PAD少u人EOSPADPADEOSEOSPAD館PAD公寄PADPADPAD登EOS方自實EOSAN上固PAD貼PAD須止7PAD樣PADPADPADPADPAD業PADPADPAD0也PAD後PAD規PADT地PADPADoPADPADEOS減刻PAD電造PAD先星投EOSPADPADEOS於PAD前PAD品覽PADPADPAD聯不版駕享EOSR奈度四PAD7PAD埠EOS力PADiPADPADPADPADPAD可PADPADPAD2失PAD市PAD之PAD只LPADPADEOSPADPADEOS餡銀PAD苗豪PAD樂EOS對EOSPADPADEOS德PAD6PAD公謀PADPADPADEOS通問車因網變片m同PAD遭PAD以能書PADEOSPADPADPADPADPAD員PADPADPAD1機PAD熱PAD8PAD並EOSPADPADEOSPADPAD8光可PAD光家PAD救網EOSEOSPADPADEOSEOSPADEOSPAD等妝PADPADPADEOS工 服EOS爾M統貶動PADEOSPAD頻線7PAD網PADPADPADPADPAD方PADPADPAD年遠PAD年PAD苗PAD訊EOSPADPADEOSPADPAD思是EOSPAD長慘PAD碳深PADEOSPADPADEOSEOSPADEOSPADa協PADPADPAD網EOSA松EOS資EOS們約要PADXPAD在EOS力PAD書PADPADPADPADPAD那PADPADPAD度突PAD貶PAD　PAD？EOSPADPADEOSPADPAD鋰信及PAD陰頂PAD長速PAD不PADPADEOSEOSPADEOSPAD臨國PADPADPAD強高lEOS網益EOS奈貼EOSPADEOSPADEOS能女PADAPADPADPADPADPAD　PADPADPADGEOSPAD4PAD羅PAD讓EOSPADPADPADPADPAD少來商PAD綜達PAD　彙PAD習PADPAD效EOSPADEOSPADa幹PADPADPAD網EOSl自強以抱市隆於PAD鋼PAD網線業PAD算PADPADPADPADPAD滿PADPADPADD　PAD賠PAD盪PADSEOSPADPADPADPADPAD樂罩界PADPAD新PAD殊加PAD助PADPAD效EOSPADEOSPAD臨官PADPADPADi高EOS相rEOS強為驅百PAD准PAD路EOS玻PADAPADPADPADPADPAD市PADPADPADP四PAD0PAD寫PAD強相PADPADPADPADPAD星針、PADPADEOSPAD兆ePAD7PADPAD效EOSPAD能PAD 提PADPADPAD習EOSEOSEOS日3機排槍　PAD勞PAD為溫造PAD算PADPADPADPADPAD角PADPADPAD達同PAD異PAD槽PAD郵習PADPADPADPADPAD舉-EOSPADPAD有PAD享nPAD可PADPAD效提PAD等PADs為PADPADPAD覽EOSEOSEOSr考亮儘困EOSPADrPAD益彙閉PAD漲PADPADPADPADPADEOSPADPADPAD6營PAD可PAD盪PADEOS推PADPADPADPADPAD持c及PADPAD成PAD防EOSPADEOSPADPAD駕爾PAD將PADo常PADPADPADEOS不EOSEOS日銀Oo來不PAD美PAD以加EOSPAD於PADPADPADPADPAD網PADPADPAD0算PAD錦PAD寫PADEOSEOSPADPADPADPADPAD官-商PADPAD輕PADEOS智PADEOSPADPAD安等PADEOSPADEOS嶸PADPADPADEOS通EOSEOSrEOSPAD確位熟PAD擺PADEOSq進PADEOSPADPADPADPADPAD兼PADPADPAD0望PAD鵬PAD鈔PADEOS進PADPADPADPADPAD提c界PADPAD台PADEOS外PADEOSPADPAD質諧PAD進PAD芝搶PADPADPAD網姻EOSEOS是EOSPAD能硬PADPAD伺PAD網碼在PAD不PADPADPADPADPAD限PADPADPAD兆書PADGPAD態PADEOS徒PADPADPADPADPAD劇救殺PADPADEOSPADEOS外PADEOSPADPAD3今PAD互PAD救度PADPADPAD強爾EOSEOS與EOSPADe驅PADPAD頻PAD作dEOSPAD樣PADPADPADPADPAD層PADPADPAD日四PAD市PADEOSPADEOSAPADPADPADPADPAD陰cEOSPADPAD建PADEOSaPAD不PADPAD生BPAD8PAD籠緊PADPADPAD公EOS書EOS步能PAD守隱PADPAD購PAD貨顧於PADEOSPADPADPADPADPAD真PADPADPAD圓電PAD擅PAD7PADEOS地PADPADPADPADPAD防r於PADPAD輕PAD書豌PADEOSPADPAD飯IPAD億PADEOSPADPADPADPAD相高PEOS存遭PAD泰逐PADPAD陳PADEOS功在PAD不PADPADPADPADPAD軍PADPADPADEOS預PAD造PAD助PAD力LPADPADPADPADPAD剩理百PADPAD台PAD達架PADEOSPADPAD成EOSPAD返PAD浮PADPADPADPAD習EOSEOSEOS陸利PAD薩上PADPAD鳴PAD不　EOSPAD樣PADPADPADPADPAD到PADPADPAD進錶PAD營PAD層PAD線EOSPADPADPADPADPAD　　供PADPADEOSPADEOS銘PAD不PADPAD%EOSPAD災PAD公PADPADPADPAD助不EOSEOS吹IPAD來逐PADPAD轉PAD減觸業PADiPADPADPADPADPAD罩PADPADPAD兆薩PAD賠PAD目PADxEOSPADPADPADPADPAD劇加、PADPAD運PAD能視PAD齡PADPAD歐提PAD已PAD司PADPADPADPAD至通EOSEOS實EOSPAD的上PADPAD制PAD筆盞萄PADEOSPADPADPADPADPAD重PADPADPAD達　PAD0PAD實PAD母EOSPADPADPADPADPAD諧公EOSPADPAD你PAD線至PAD小PADPAD計網PADEOSPADoPADPADPADPAD　姻EOSEOS享網PAD簡逐PADPAD造PAD轉F面PADEOSPADPADPADPADPAD石PADPADPAD步四PAD監PAD畫PAD亮EOSPADPADPADPADPAD碩強力PADPADNPADEOS皇PAD6PADPAD辨穩PAD進PADoPADPADPADPADA權書EOSEOSPADPAD平上PADPAD拒PAD效憂在PAD不PADPADPADPADPAD提PADPADPADc同PAD放PAD慘PADEOSEOSPADPADPADPADPAD持uEOSPADPAD才PAD於EOSPAD可PADPAD%瑪PAD和PADEOSPADPADPADPAD寬整PEOS網PADPAD 逐PADPAD先PAD劇黃EOSPAD樣PADPADPADPADPAD網PADPADPAD灣營PAD0PAD8PAD倫相PADPADPADPADPAD17於PADPAD董PAD百EOSPADEOSPADPAD利DPADEOSPAD青PADPADPADPAD疲EOSEOSEOS取PADPAD製上PADPAD查PAD諧事EOSPAD渺PADPADPADPADPAD購PADPADPAD展算PAD監PAD　PAD全習PADPADPADPADPAD0奈百PADPAD端PAD盞網PAD不PADPAD可rPADEOSPAD證PADPADPADPAD2高EOSEOS寶PADPAD辨逐PADPAD雲PAD　業EOSPADEOSPADPADPADPADPADbPADPADPAD　EOSPAD放PAD未PADPAD推PADPADPADPADPAD奈信供PADPAD黨PADEOS義PAD升PADPAD生題PADEOSPAD鉅PADPADPADPAD溫EOSEOSEOS是PADPAD朝上PADPAD端PAD春者、PAD不PADPADPADPADPADEOSPADPADPAD內EOSPAD0PAD來PADPADEOSPADPADPADPADPAD全PAD力PADPAD今PAD能EOSPAD層PADPAD飯德PADEOSPADFPADPADPADPAD蘭不EOSEOSEOSPADPADEOS逐PADPAD榜PAD額外EOSPAD樣PADPADPADPADPADEOSPADPADPADiEOSPAD監PAD然PADPAD進PADPADPADPADPAD球PADEOSPADPAD達PAD習EOSPAD6PADPAD成EOSPADEOSPAD東PADPADPADPAD術通EOSEOSEOSPADPAD進上PADPADaPAD季重相PAD渺PADPADPADPADPAD層PADPADPAD搜EOSPAD放PAD國PADPAD徒PADPADPADPADPAD渺PAD於PADPAD晶PAD助EOSPADEOSPADPADPADEOSPAD能PADFPADPADPADPAD全工書EOS網PADPAD盪度PADPAD雲PAD席只習PADEOSPADPADPADPADPAD目PADPADPAD升EOSPAD高PAD界PADPADAPADPADPADPADPADEOSPADEOSPADPADEOSPAD領不PADEOSPADPADPADEOSPAD等PADoPADPADPADPAD餡EOS強EOS強PADPAD季mPADPAD端PAD智匯塑PAD不PADPADPADPADPAD製PADPADPADtEOSPAD機PADEOSPADPAD地PADPADPADPADPAD7PAD力PADPAD於PADO欲PADEOSPADPADPADEOSPAD將PADEOSPADPADPADPADEOS高iEOS微PADPAD席貶PADPAD測PAD室割書PAD減PADPADPADPADPAD辨PADPADPAD工EOSPAD箭PAD網PADPADLPADPADPADPADPAD許PADPADPADPAD8PADEOS主PAD不PADPADPADEOSPADEOSPAD青PADPADPADPADEOSEOS效EOS穆PADPAD7約PADPADxPAD樁邀7PADEOSPADPADPADPADPAD潮PADPADPAD略構PAD皇PAD魅PADPADEOSPADPADPADPADPAD他PADPADPADPAD達PAD熱視PAD習PADPADPAD提PAD進PAD證PADPADPADPAD鼻EOS官EOS有PADPAD自貼PADPAD什PAD凸開就PAD不PADPADPADPADPADEOSPADPADPAD、視PADEOSPAD燃PADPADEOSPADPADPADPADPAD防PADPADPADPAD熱PAD板也PAD助PADPADPAD爾PAD互PAD鉅PADPADPADPADEOS不　EOS主PADPAD打%PADPADxPAD1顧幹PAD樣PADPADPADPADPAD零PADPADPAD化營PAD此PAD勝PADPADEOSPADPADPADPADPADSPADPADPADPAD員PAD滴深PADEOSPADPADPADEOSPAD8PADFPADPADPADPAD效通戰EOSePADPAD至充PADPAD里PAD1功零PADiPADPADPADPADPAD效PADPADPAD輕算PAD鵬PAD來PADPADEOSPADPADPADPADPAD劇PADPADPADPAD方PAD低EOSPADEOSPADPADPAD提PAD億PAD東PADPADPADPAD登姻eEOSEOSPADPAD市達PADPAD法PAD%EOS透PADEOSPADPADPADPADPAD技PADPADPAD身唯PAD網PAD然PADPADEOSPADPADPADPADPADRPADPADPADPAD案PAD斷EOSPADEOSPADPADPAD網PAD返PAD芝PADPADPADPADI爾 EOSEOSPADPAD0繼PADPAD造PAD歐EOSEOSPADEOSPADPADPADPADPAD術PADPADPAD季像PAD監PAD幕PADPAD相PADPADPADPADPAD千PADPADPADPAD新PAD迪EOSPAD不PADPADPADrPAD災PAD直PADPADPADPAD超EOSAEOS網PADPAD億紋PADPAD雲PADPAD、於PAD不PADPADPADPADPADEOSPADPADPAD席義PAD放PAD苗PADPAD習PADPADPADPADPAD賈PADPADPADPAD巢PAD：網PAD習PADPADPAD育PAD已PAD包PADPADPADPAD嚇高cEOS強PADPAD美拚PADPAD端PADPAD員在PAD減PADPADPADPADPAD網PADPADPAD季家PAD0PAD虎PADPAD推PADPADPADPADPAD長PADPADPADPAD體PAD問爾PAD助PADPADPAD構PADEOSPAD1PADPADPADPAD溫EOS傳EOS效PADPAD升體PADPAD測PADPAD能EOSPAD然PADPADPADPADPAD斯PADPADPAD總伏PADgPAD景PADPADEOSPADPADPADPADPAD職PADPADPADPAD上PAD用資PAD委PADPADPAD銀PAD進PAD談PADPADPADPADEOSEOS回EOS強PADPAD行EOSPADPAD試PADPAD臨睛PAD略PADPADPADPADPAD美PADPADPAD　訊PAD財PAD6PADPAD進PADPADPADPADPAD溫PADPADPADPAD止PAD坐場PAD扳PADPADPADBPAD和PAD鷹PADPADPADPAD力不 EOS微PADPAD群此PADPADEOSPADPAD萄EOSPAD網PADPADPADPADPAD技PADPADPAD亂機PAD可PAD系PADPAD徒PADPADPADPADPAD錯PADPADPADPAD標PAD造力PAD齡PADPADPADIPADEOSPAD前PADPADPADPADEOS通代EOS穆PADPAD蘋體PADPAD於PADPAD少於PAD子PADPADPADPADPAD術PADPADPAD1往PAD能PAD幕PADPADAPADPADPADPADPAD1PADPADPADPAD鳴PADvEOSPAD再PADPADPAD購PADEOSPAD演PADPADPADPAD進姻動EOS有PADPAD寫彙PADPAD讓PADPAD紅書PAD主PADPADPADPADPAD創PADPADPAD氣也PAD蜂PAD苗PADPAD地PADPADPADPADPAD績PADPADPADPAD前PAD工EOSPAD度PADPADPAD品PADEOSPAD1PADPADPADPAD不爾獲EOS是PADPAD薩康PADPAD習PADPADT7PADAPADPADPADPADPAD新PADPADPAD曄失PAD回PAD虎PADPADLPADPADPADPADPAD1PADPADPADPAD達PADEOS網PADLPADPADPADEOSPADEOSPAD刻PADPADPADPAD兌EOS機EOSEOSPADPAD　來PADPAD在PADPAD系提PAD算PADPADPADPADPAD重PADPADPAD總機PAD果PAD景PADPADEOSPADPADPADPADPAD煞PADPADPADPADEOSPAD豌深PAD港PADPADPADDPADEOSPAD風PADPADPADPAD克高lEOSEOSPADPAD制菲PADPAD再PADPAD統盪PAD漲PADPADPADPADPAD到PADPADPAD念遠PAD李PADKPADPADEOSPADPADPADPADPAD劇PADPADPADPAD於PAD替EOSPAD別PADPADPADrPAD能PAD直PADPADPADPAD力EOScEOS網PADPAD1逐PADPAD勞PADPAD老提PADEOSPADPADPADPADPADEOSPADPADPAD零突PAD筆PAD加PADPADEOSPADPADPADPADPADRPADPADPADPAD8PAD電EOSPAD再PADPADPAD題PAD等PAD包PADPADPADPAD如EOStEOS強PADPAD洪上PADPAD持PADPAD舊LPAD不PADPADPADPADPAD網PADPADPAD子EOSPAD碼PAD信PADPADEOSPADPADPADPADPAD攀PADPADPADPAD幣PAD出不PAD月PADPADPADEOSPAD將PAD進PADPADPADPAD準不EOSEOS微PADPAD6逐PADPAD啟PADPAD隱EPAD樣PADPADPADPADPAD購PADPADPAD、　PAD財PAD池PADPADEOSPADPADPADPADPAD於PADPADPADPAD力PAD開熟PAD可PADPADPADDPADEOSPAD1PADPADPADPADEOS通EOSEOS穆PADPAD默上PADPAD克PADPAD憂OPADiPADPADPADPADPAD抬PADPADPAD務四PAD千PAD上PADPAD相PADPADPADPADPAD全PADPADPADPAD達PAD望不PAD能PADPADPADrPAD進PAD指PADPADPADPADEOS姻EOSEOS有PADPAD傳逐PADPAD在PADPADEOS億PADEOSPADPADPADPADPAD材PADPADPAD公同PAD一PAD發PADPAD習PADPADPADPADPAD球PADPADPADPAD繁PAD路ePAD與PADPADPAD材PAD互PADaPADPADPADPADPAD爾EOSEOS主PADPAD料上PADPAD國PADPADEOStPAD網PADPADPADPADPAD首PADPADPAD櫃營PAD轉PAD展PADPAD推PADPADPADPADPAD色PADPADPADPAD器PAD路架PAD0PADPADPAD瑪PAD8PAD直PADPADPADPADPADEOSEOSEOSePADPADPAD逐PADPAD企PADPAD能？PAD減PADPADPADPADPAD並PADPADPAD　算PAD今PAD期PADPADEOSPADPADPADPADPAD劇PADPADPADPAD專PAD種騰PADEOSPADPADPADDPAD億PAD包PADPADPADPADPAD高EOSEOSEOSPADPADPAD上PADPAD業PADPAD線等PADEOSPADPADPADPADPAD再PADPADPAD思漲PAD果PADEOSPADPAD進PADPADPADPADPADRPADPADPADPADEOSPAD便碳PADEOSPADPADPADEOSPAD返PAD中PADPADPADPADPADEOSEOSEOSEOSPADPADPAD逐PADPAD找PADPADEOS朝PAD不PADPADPADPADPAD可PADPADPAD諧書PAD限PAD進PADPAD徒PADPADPADPADPADEOSPADPADPADPAD於PAD殺樺PADEOSPADPADPADDPAD災PAD在PADPADPADPADPADEOS書EOS網PADPADPAD上PADPAD元PADPAD不EOSPAD樣PADPADPADPADPAD求PADPADPAD\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
